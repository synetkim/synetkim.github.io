<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="">
        <meta name="author" content="">
        <link rel="shortcut icon" href="../docs-assets/ico/favicon.png">

        <title>Suyoun Kim contact</title>

        <!-- Bootstrap core CSS -->
        <link href="../dist/css/bootstrap.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="../examples/starter-template/starter-template.css" rel="stylesheet">
        <link href="../simple_resume_template.css" rel="stylesheet">
    </head>
    <!-- NAVBAR
    ================================================== -->
    <body>
        <div class="navbar-wrapper">
            <div class="container">

                <div class="navbar navbar-inverse navbar-static-top" role="navigation">
                    <div class="container">
                        <div class="navbar-header">
                            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                                <span class="sr-only">Toggle navigation</span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                            </button>
                            <a class="navbar-brand" href="#">Suyoun Kim</a>
                        </div>
                        <div class="navbar-collapse collapse">
                            <ul class="nav navbar-nav">
                                <li><a href="../#">About</a></li>
                                <li class="active"><a href="../projects">Projects</a></li>
                                <li><a href="../courses">Courses</a></li>
                            </ul>
                        </div>
                    </div>
                </div>

            </div>
        </div>
        <!--
        ============================================================= -->
        <div class="container">
            <div class="jumbotron">
                <div class="row featurette">
                    <div class="col-md-4">
                        <figure>
                        <img class="img-circle img-responsive" alt="" src="../contents/photo_suyoun.png">
                        </figure>
                    </div>
                    <div class="col-md-8">
                        <h2><b>Suyoun Kim</b></h2>
                        <h4>
                            Ph.D Student<br>
                            Carnegie Mellon University <br> 
                            Electrical and Computer Engineering <br><br>
                            <b>Contact</b> <br>
                            Email: suyoun@cmu.edu  <br>
                        </h4>
                        <a class="btn btn-default btn-primary"
                            href="contents/SuyounKim_CV.pdf" download
                            role="button">Download my CV &raquo;</a>  <br>

                        <a target="_blank" href="https://www.linkedin.com/in/suyoun" role="button" class="btn btn-default btn-li"><i class="fa fa-linkedin left"></i> Linkedin</a>
                        <a target="_blank" href="https://github.com/synetkim/" role="button" class="btn btn-default btn-git"><i class="fa fa-github left"></i> Github</a>
                        <a target="_blank"
                            href="https://scholar.google.com/citations?user=zTkuGlEAAAAJ&hl=en"
                            role="button" class="btn btn-default btn-gplus"><i
                                class="fa fa-google-plus left"></i> GoogleScholar</a>
                    </div>
                </div>
            </div>

            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive"
                    src="../contents/ctc_attention.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">
                        Joint CTC-Attention based End-to-End Speech Recognition
                        using Multi-task Learning
                        <span
                            class="text-muted">Deep Learning, End-to-End Speech
                            Recognition</span></h2>
                    <p>
                    Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the attention model has shown poor results especially in noisy condition and is hard to be trained in the initial training stage with long input sequences, as compared with CTC. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 6.6-10.3% relative improvements in Character Error Rate (CER).

                    </p>
                    <p><a  target="_blank" class="btn btn-default btn-primary"
                        href="https://arxiv.org/abs/1609.06773" 
                        role="button">arXiv &raquo;</a>    </p>
                </div>
            </div>

            <hr class="featurette-divider">
            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive"
                    src="../contents/multiattend.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">
                        RECURRENT MODELS FOR
                        AUDITORY ATTENTION IN MULTI-MICROPHONE
                        DISTANCE SPEECH RECOGNITION <span
                            class="text-muted">Deep Learning, Robust Speech
                            Recognition</span></h2>
                    <p>
                    Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multi-channel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-tosequence learning, however, no attention mechanisms have been applied to learn multiple inputs which may be asynchronous and non-stationary. We evaluate our neural attention model on a subset of the CHiME-3 challenge task, and we show that the model achieves comparable performance to beamforming using a purely data-driven method.
                    </p>
                    <p><a  target="_blank" class="btn btn-default btn-primary"
                        href="https://arxiv.org/abs/1511.06407" 
                        role="button">arXiv &raquo;</a>    </p>
                </div>
            </div>
            <hr class="featurette-divider">
            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive"
                    src="../contents/neat.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">Environmental Noise
                        Embeddings for Robust Speech Recognition <span
                            class="text-muted">Deep Learning, Robust Speech
                            Recognition</span></h2>
                    <p>
                    We propose a novel deep neural network architecture for
                    speech recognition that explicitly employs knowledge of the
                    background environmental noise within a Deep Neural
                    Network Acoustic Model. A bottleneck Deep Neural Network is
                    used to estimate the acoustic environment in which the
                    system in being used. The discriminative embedding
                    generated at the bottleneck layer of this network is then
                    concatenated with traditional acoustic features as input to
                    a Deep Neural Network acoustic model. Using simulated
                    acoustic environments we show that the proposed approach
                    significantly improves speech recognition accuracy in
                    noisy and highly reverberant environments, outperforming
                    multi-condition training and multi-task learning for this
                    task.
                    </p>
                    <p><a  target="_blank" class="btn btn-default btn-primary"
                        href="https://arxiv.org/abs/1601.02553" 
                        role="button">arXiv &raquo;</a>    </p>
                </div>
            </div>


            <hr class="featurette-divider">
            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive"
                    src="../contents/tdl.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">MULTIMODAL TRANSFER DEEP LEARNING FOR AUDIO VISUAL RECOGNITION <span
                            class="text-muted">Deep Learning, Transfer Learning</span></h2>
                    <p>
                    We propose a multimodal deep learning framework that can
                    transfer the knowledge obtained from a single-modal
                    neural network to a network with a different modality. For
                    instance, we show that we can leverage the speech data to
                    fine-tune the network trained for video recognition, given
                    an initial set of audio-video parallel dataset within the
                    same semantics. Our approach learns the analogy-preserving
                    embeddings between the abstract representations learned
                    from each network, allowing for semantics-level transfer
                    or reconstruction of the data among different modalities.
                    Our method is thus specifically useful when one of the
                    modalities is more scarce in labeled data than other
                    modalities. While we mainly focus on ap- plying transfer
                    learning on the audio-visual recognition task as an
                    application of our approach, our framework is flexible and
                    thus can work with any multimodal datasets. In this
                    work-in-progress report, we show our preliminary results on
                    the AV-Letters dataset.
                    </p>
                    <p><a  target="_blank" class="btn btn-default btn-primary"
                        href="https://arxiv.org/abs/1412.3121" download
                        role="button">Report &raquo;</a>    </p>
                </div>
            </div>


            <hr class="featurette-divider">
            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive" src="../contents/lyric.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">Lyric Recognition <span class="text-muted">Speech Recognition</span></h2>
                    <p>During the final project in Machine Learning for Signal Processing, our team built a Lyric Recognition System which takes a segment of a song as input and generates the lyrics of that line as an output. The system is a novel application of speech recognition techniques, and it was a challenging project because we had to overcome issues such as overlaid music, stylized voices, mispronunciations, and irregular grammar rules, which are not found in conventional speech recognition problems. As a solution, we added Chroma features, applied lyric specific fillers, built a lyric specific language model,and built HMM acoustic models with different numbers of Gaussian mixtures. These approaches allowed us to reduce word error rate (WER) by approximately 30% compared to the baseline. Lyric recognition was my first foray into speech recognition, and I found the experience to be highly enjoyable. I plan to continue this work, specifically by adapting feature extraction from a deep neural network and applying a seam carving technique to remove vowel extensions in singing voices.</p>
                    <p><a class="btn btn-default btn-primary"
                        href="../contents/lyric_recognition.pdf" download
                        role="button">Report &raquo;</a>    </p>
                </div>
            </div>


            <hr class="featurette-divider">

            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive" src="../contents/ppi.jpg" alt="Generic placeholder image">
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">Prediction Human Protein-protein Interaction <span class="text-muted">Computational Biology, Machine Learning</span></h2>
                    <p>I have been conducting research on several computational biology projects with Professor Madhavi Ganapathiraju. I focused on overcoming the scarcity of labeled data that is common in human protein-protein interaction prediction. As a solution, I applied the transfer learning approach, where the goal is to improve learning in the target domain by leveraging knowledge from a source domain. In this work, I transferred generic features such as Gene Ontology annotations from other organisms for cases where the annotations were not available in the human data, resulting in improved accuracy. In addition, I transferred the computational models across species to predict interactions that may not be captured by the model trained with the few known interactions available. For example, known interactions from flies, mice, rats, worms, and yeast were used to predict the interaction of proteins in the human dataset. I have applied several regularization methods based on biological hypotheses, such as giving different weights to each classifier based on the evolutionary distance. </p>
                    <p>
                    <a class="btn btn-default btn-primary"
                        href="../contents/ltiopenhouse_poster.pdf" download
                        role="button">Poster 1 &raquo;</a>
                    <a class="btn btn-default btn-primary"
                        href="../contents/ppi_poster_sykim.pdf" download
                        role="button">Poster 2 &raquo;</a>
                    </p>
                </div>
            </div>

            <hr class="featurette-divider">

            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive" src="../contents/repositioning.jpg" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">Drug Repositioning <span class="text-muted">Computational Biology, Graph-based Algorithm</span></h2>
                    <p>Another ongoing research project towards my research goal is a drug repositioning project under Professor Ganapathiraju, with a focus on discovering new relationships between drugs and diseases by applying ProphNet, a graph-based algorithm. I proposed to incorporate the side effect information of drugs into the algorithm to help discover new diseases that the drug can treat effectively. The most significant opportunity that came from this experience was learning how to build a distributed machine learning system for a large dataset and to implement complex algorithms that can handle the big data efficiently.</p>
                    <p>
                    <a class="btn btn-default btn-primary"
                        href="../contents/drug_talk.pdf" download
                        role="button">Slide &raquo;</a>
                    <!--<a class="btn btn-default btn-primary"
                        href="../contents/drug.pdf" download
                        role="button">Report &raquo;</a> -->
                    </p>
                </div>
            </div>
            <hr class="featurette-divider">


            <div class="row featurette">
                <div class="col-md-3">
                    <img class="featurette-image img-responsive" src="../contents/swe.png" >
                </div>
                <div class="col-md-9">
                    <h2 class="featurette-heading">Electronic Medical Record system <span class="text-muted">Natural Lanuage Processing, Unstructured Information Management applications (UIMA framework)</span></h2>
                    <p>In the Intelligent Information Systems Project (IISP) class, our team built an Electronic Medical Record (EMR) system with Professor Eric Nyberg which identifies disease codes from EMR documents by applying several NLP techniques with the UIMA framework. </p>
                    <p><a class="btn btn-default btn-primary"
                        href="../contents/emr.pdf" download
                        role="button">Report &raquo;</a>    </p>
                </div>


            </div> <!-- row -->
            <hr class="featurette-divider">
            <footer>
            <p class="pull-right"><a href="#">Back to top</a></p>
            <p>&copy; Suyoun Kim 2016 &middot; <a href="#">Privacy</a> &middot; <a href="#">Terms</a></p>
            </footer>

        </div>
    </div><!-- /.container -->
</div>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="dist/js/bootstrap.min.js"></script>
<script src="docs-assets/js/holder.js"></script>
  </body>
</html>
